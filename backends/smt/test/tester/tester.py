from collections import Counter, OrderedDict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union

import torch
from torch.export import export, ExportedProgram
from torch.testing import FileCheck
from abc import ABC, abstractmethod
import random

from executorch.backends.smt.partition.smt_partitioner import SMTPartitioner

from executorch.exir import (
    EdgeCompileConfig,
    EdgeProgramManager,
    ExecutorchBackendConfig,
    ExecutorchProgramManager,
    to_edge,
    to_edge_transform_and_lower,
)

from executorch.exir.backend.partitioner import Partitioner


class Stage(ABC):
    """
    Interface for a Stage in the PT2.0 lowering pipeline
    """

    @abstractmethod
    def run(self, artifact, inputs):
        """
        Executes this stage, generates the 'artifact', for later stages.
        """
        pass

    @property
    @abstractmethod
    def artifact(self):
        """
        Returns the artifact generated by this stage. To be used by the next stage in the pipeline.
        """
        pass

    @property
    @abstractmethod
    def graph_module(self):
        """
        Return the artifact's graph module for this stage
        """
        pass

    def run_artifact(self, inputs):
        """
        Returns the output of calling the artifact generated by this stage with inputs
        """
        if isinstance(self.artifact, ExportedProgram):
            return self.artifact(*inputs)
        else:
            return self.artifact.exported_program().module()(*inputs)

    # Debug Tools for stages
    def artifact_str(self):
        """
        Return string printable artifact for this stage
        """
        if isinstance(self.artifact, EdgeProgramManager):
            return self.artifact.exported_program()
        return self.artifact

    def stage_banner(self):
        """
        Returns banner string for this stage
        """
        return "#" * 36 + " " + str(self.__class__.__name__) + " " + "#" * 36 + "\n"

    def dump_artifact(self, path_to_dump: Optional[str]):
        """
        Dumps string printable artifact to path. If path_to_dump, then it is printed to terminal
        """
        if path_to_dump:
            with open(path_to_dump, "a") as fp:
                fp.write(str(self.stage_banner() + "\n"))
                fp.write(str(self.artifact_str()))
        else:
            print(self.stage_banner() + "\n")
            print(self.artifact_str())


_stages_: Dict[str, Stage] = {}


def register_stage(stage: Stage):
    """
    Register a Stage to be used in the Tester.
    """
    assert isinstance(stage, type)
    name = stage.__qualname__
    if name in _stages_:
        raise RuntimeError(f"Duplicate stage in Tester, {name}")
    _stages_[name] = stage
    return stage


@register_stage
class Export(Stage):
    def __init__(self, dynamic_shapes: Optional[Tuple[Any]] = None):
        self.exported_program = None
        self.dynamic_shapes = dynamic_shapes

    def run(
        self,
        artifact: torch.nn.Module,
        inputs: Tuple[torch.Tensor],
    ) -> None:
        self.exported_program = export(
            artifact, inputs, dynamic_shapes=self.dynamic_shapes
        )

    @property
    def artifact(self) -> ExportedProgram:
        return self.exported_program

    @property
    def graph_module(self) -> str:
        return self.exported_program.graph_module


@register_stage
class ToEdge(Stage):
    def __init__(self, edge_compile_config: Optional[EdgeCompileConfig] = None):
        self.edge_compile_conf = edge_compile_config
        self.edge_dialect_program = None

    def run(self, artifact: ExportedProgram, inputs=None) -> None:
        self.edge_dialect_program = to_edge(
            artifact, compile_config=self.edge_compile_conf
        )

    @property
    def artifact(self) -> EdgeProgramManager:
        return self.edge_dialect_program

    @property
    def graph_module(self) -> str:
        return self.edge_dialect_program.exported_program().graph_module


@register_stage
class ToEdgeTransformAndLower(Stage):
    def __init__(
        self,
        partitioners: Optional[List[Partitioner]] = None,
        edge_compile_config: Optional[EdgeCompileConfig] = None,
    ):
        self.partitioners = partitioners or [SMTPartitioner()]
        self.edge_compile_conf = edge_compile_config
        self.edge_dialect_program = None

    def run(self, artifact: ExportedProgram, inputs=None) -> None:
        artifact_to_run = copy.deepcopy(artifact)
        self.edge_dialect_program = to_edge_transform_and_lower(
            artifact_to_run,
            compile_config=self.edge_compile_conf,
            partitioner=self.partitioners,
        )

    @property
    def artifact(self) -> EdgeProgramManager:
        return self.edge_dialect_program

    @property
    def graph_module(self) -> str:
        return self.edge_dialect_program.exported_program().graph_module


@register_stage
class Partition(Stage):
    def __init__(self, partitioner: Optional[Partitioner] = None):
        self.partitioner = partitioner or SMTPartitioner()
        self.delegate_module = None

    def run(self, artifact: EdgeProgramManager, inputs=None):
        with validation_disabled():
            self.delegate_module = artifact
            self.delegate_module = self.delegate_module.to_backend(self.partitioner)

    @property
    def artifact(self) -> EdgeProgramManager:
        return self.delegate_module

    @property
    def graph_module(self) -> str:
        return self.delegate_module.exported_program().graph_module


class SmtTester:

    def __init__(
        self,
        module: torch.nn.Module,
        example_inputs: Tuple[torch.Tensor, ...],
        dynamic_shapes: Optional[Any] = None,
    ):
        module.eval()
        self.original_module = module
        self.example_inputs = example_inputs
        self.dynamic_shapes = dynamic_shapes
        self.stages: Dict[str, Stage] = OrderedDict.fromkeys(list(_stages_.keys()))
        self.pipeline = {
            self.stage_name(Export): [
                self.stage_name(ToEdge),
                self.stage_name(ToEdgeTransformAndLower),
            ],
            self.stage_name(ToEdgeTransformAndLower): [],
            self.stage_name(ToEdge): [
                self.stage_name(Partition),
            ],
            self.stage_name(Partition): [],
        }
        assert all(
            stage in self.pipeline for stage in self.stages
        ), "Invalid Tester internal state!"

        self.artifacts: Dict[str, Any] = OrderedDict()

        self.cur: str = ""

        self.reference_outputs = None
        self.stage_output = None

    def generate_random_inputs(self):
        # Get shapes of inputs
        input_shapes = []
        if self.dynamic_shapes is None:
            for tensor_arg in self.example_inputs:
                assert isinstance(tensor_arg, torch.Tensor)
                input_shapes.append(tensor_arg.shape)
        else:
            # Random shapes depending on dynamic shape constraint
            dim_name_to_size = {}
            for arg_idx in range(len(self.example_inputs)):
                assert isinstance(self.example_inputs[arg_idx], torch.Tensor)
                ex_shape = list(self.example_inputs[arg_idx].shape)
                dynamic_dim_spec = self.dynamic_shapes[arg_idx]
                for dim_idx, dim_spec in dynamic_dim_spec.items():
                    assert dim_idx < len(ex_shape)
                    if isinstance(dim_spec, torch.export.dynamic_shapes._DerivedDim):
                        # derived dims are of the form {0: 2 * torch.export.Dim() // 2}
                        # The root contains the min/max of the export dim and fn contains
                        # the function to compute the derived dim.
                        dim_spec = dim_spec.root
                        fn = dim_spec.fn
                    elif isinstance(dim_spec, torch.export.dynamic_shapes._Dim):
                        # Not derived dim so fn is just itself
                        def fn(x):
                            return x

                    else:
                        raise RuntimeError(
                            f"Expected Dynamic Dims to be of type _DerivedDim or _Dim but got {type(dim_spec)}"
                        )
                    dim_name = dim_spec.__name__
                    if dim_name not in dim_name_to_size:
                        upper_bound = min(
                            dim_spec.max, 1000
                        )  # unbounded int max is too large
                        lower_bound = (
                            dim_spec.min if dim_spec.min >= 2 else 1
                        )  # 0/1 specialization means dim_spec.min can never be 1
                        dim_name_to_size[dim_name] = fn(
                            random.randint(lower_bound, upper_bound)
                        )
                    ex_shape[dim_idx] = dim_name_to_size[dim_spec.__name__]
                input_shapes.append(torch.Size(ex_shape))
        # create random tensor inputs with the shapes given above:
        random_inputs = []
        for arg_idx in range(len(self.example_inputs)):
            random_inputs.append(
                torch.randn(input_shapes[arg_idx]).to(
                    dtype=self.example_inputs[arg_idx].dtype
                )
            )

        yield tuple(random_inputs)

    @staticmethod
    def stage_name(stage) -> str:
        t = stage if isinstance(stage, type) else type(stage)
        return t.__qualname__

    def _pre(self, stage):
        name: str = self.stage_name(stage)
        assert isinstance(name, str) and name in self.stages and not self.stages[name]

        last_artifact = self.original_module
        if self.cur:
            assert self.cur in self.pipeline, f"Invalid state: {self.cur}"
            allowed_next_stages = self.pipeline[self.cur]
            assert name in allowed_next_stages, f"Invalid next stage: {name}"
            last_artifact = self.get_artifact()
        self.cur = name
        return last_artifact

    def _post(self, stage):
        name = self.stage_name(stage)
        assert name in self.stages
        self.stages[name] = stage

    def _run_stage(self, stage_instance, inputs=None):
        assert isinstance(stage_instance, Stage)
        prev_stage_artifact = self._pre(stage_instance)
        stage_instance.run(prev_stage_artifact, inputs=inputs)
        self._post(stage_instance)
        return self

    def export(self, export_stage: Optional[Export] = None):
        return self._run_stage(
            export_stage or Export(dynamic_shapes=self.dynamic_shapes),
            self.example_inputs,
        )

    def check_count(self, input: Dict[Any, int]):
        for key, count in input.items():
            FileCheck().check_count(key, count, exactly=True).run(
                self.stages[self.cur].graph_module.code
            )
        return self

    def to_edge_transform_and_lower(
        self, to_edge_and_transform_stage: Optional[ToEdgeTransformAndLower] = None
    ):
        return self._run_stage(to_edge_and_transform_stage or ToEdgeTransformAndLower())

    def encode_smt(self):
        if "export" not in self.artifacts:
            raise RuntimeError("Must run .export() before .encode_smt()")

        ep: ExportedProgram = self.artifacts["export"]

        smt_encoding = f"<FakeSmtEncoding for {ep}>"
        self.artifacts["smt_encoding"] = smt_encoding
        return self

    def solve_smt(self):
        if "smt_encoding" not in self.artifacts:
            raise RuntimeError("Must run .encode_smt() before .solve_smt()")

        encoding = self.artifacts["smt_encoding"]

        solver_result = "<FakeSolverResult: sat>"
        self.artifacts["smt_result"] = solver_result
        return self

    def run_method_and_compare_outputs(
        self,
        tolerance: float = 1e-4,
    ):
        if self.reference_outputs is None:
            raise RuntimeError(
                "No reference outputs to compare with. Did you .export()?"
            )
        pass

        return self

    def check(self, patterns: list[str]):
        if "export" not in self.artifacts:
            raise RuntimeError("Must run .export() before .check()")

        code_str = self.artifacts["export"].graph_module.code
        for pat in patterns:
            if pat not in code_str:
                raise AssertionError(f"Pattern '{pat}' not found in exported code.")
        return self

    def check_not(self, patterns: list[str]):
        if "export" not in self.artifacts:
            raise RuntimeError("Must run .export() before .check_not()")

        code_str = self.artifacts["export"].graph_module.code
        for pat in patterns:
            if pat in code_str:
                raise AssertionError(f"Pattern '{pat}' was unexpectedly found in code.")
        return self

    def assert_no_unsupported_ops(self):
        if "export" not in self.artifacts:
            raise RuntimeError("Must run .export() first.")
        return self
